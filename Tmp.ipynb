{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09bcda4a-6954-421a-9bbc-2c3a3e93dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset (assuming CSV format)\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv('Daily Data (1).csv')\n",
    "    \n",
    "    # Create datetime index\n",
    "    df['date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.drop(columns=['Year', 'Month', 'Day'], inplace=True)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Forward fill missing values (alternative: interpolate)\n",
    "    df.ffill(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Feature engineering\n",
    "def add_features(df, window_size=7):\n",
    "    # Rolling statistics for solar features\n",
    "    df['solar_flare_ma'] = df['Solar Flare'].rolling(window=window_size).mean()\n",
    "    df['sunspot_ma'] = df['Sunspot Number'].rolling(window=window_size).mean()\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 2, 3, 7]:\n",
    "        df[f'solar_flare_lag_{lag}'] = df['Solar Flare'].shift(lag)\n",
    "        df[f'sunspot_lag_{lag}'] = df['Sunspot Number'].shift(lag)\n",
    "    \n",
    "    # Seasonal features\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "    df['sin_day'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    df['cos_day'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "    # Drop rows with NaN created by rolling/lag features\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Normalization\n",
    "def normalize_data(df, target_col='LST'):\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[[target_col]]\n",
    "    \n",
    "    # Normalize features (MinMax for LST, Standard for others)\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(X)\n",
    "    \n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y)\n",
    "    \n",
    "    return X_scaled, y_scaled, feature_scaler, target_scaler\n",
    "\n",
    "# Create sequences for time series\n",
    "def create_sequences(X, y, seq_length=30):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9e7f5d1-fbaa-457a-b67e-6bb835c954b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class LSTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Hybrid CNN-LSTM Model with Attention\n",
    "class LSTPredictionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.3):\n",
    "        super(LSTPredictionModel, self).__init__()\n",
    "        \n",
    "        # CNN for local pattern extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_size, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Regression head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(64 + hidden_size, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # CNN branch\n",
    "        cnn_input = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
    "        cnn_out = self.cnn(cnn_input)\n",
    "        \n",
    "        # LSTM branch\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden_size)\n",
    "        \n",
    "        # Attention\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([cnn_out, context_vector], dim=1)\n",
    "        \n",
    "        # Regression\n",
    "        output = self.regressor(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21ee54a6-6e4f-4fea-9bb4-8ffb2b8d17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Learning rate scheduling (continues even without early stopping)\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    # Load best model after all epochs complete\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69f7d00e-fa19-4896-9571-5d9879db2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, target_scaler):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    y_true, y_pred = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            outputs = model(X_test)\n",
    "            \n",
    "            y_true.extend(y_test.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_true = target_scaler.inverse_transform(np.array(y_true).reshape(-1, 1))\n",
    "    y_pred = target_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1))\n",
    "    \n",
    "    # Flatten arrays for easier calculations\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = np.mean((y_true - y_pred) ** 2)  # Mean Squared Error\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    \n",
    "    print('\\n========== Evaluation Metrics ==========')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'RMSE: {np.sqrt(mse):.4f}')\n",
    "    print(f'R² Score: {r2:.4f}')\n",
    "    print(f'Pearson Correlation: {correlation:.4f}\\n')\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Actual vs Predicted Line Plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(y_true, label='Actual LST')\n",
    "    plt.plot(y_pred, label='Predicted LST', alpha=0.7)\n",
    "    plt.title('Actual vs Predicted Land Surface Temperature')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Temperature')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Scatter plot with regression line\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.regplot(x=y_true, y=y_pred, line_kws={'color': 'red'})\n",
    "    plt.xlabel('Actual LST')\n",
    "    plt.ylabel('Predicted LST')\n",
    "    plt.title(f'Predicted vs Actual (r = {correlation:.2f})')\n",
    "    \n",
    "    # 3. Error distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    errors = y_true - y_pred\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Prediction Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    \n",
    "    # 4. Correlation matrix (if multiple outputs existed)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    corr_matrix = np.corrcoef(np.vstack([y_true, y_pred]))\n",
    "    sns.heatmap(corr_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                xticklabels=['Actual', 'Predicted'],\n",
    "                yticklabels=['Actual', 'Predicted'])\n",
    "    plt.title('Correlation Regression Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227340f5-8294-4212-b797-594633c2f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "LST               0\n",
      "Solar Flare       0\n",
      "Sunspot Number    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Redwan Ahmed Tamim\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/2000: 100%|██████████| 88/88 [00:08<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1434, Val Loss: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2000: 100%|██████████| 88/88 [00:09<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0704, Val Loss: 0.0842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/2000: 100%|██████████| 88/88 [00:07<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0527, Val Loss: 0.0823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/2000: 100%|██████████| 88/88 [00:10<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0428, Val Loss: 0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/2000: 100%|██████████| 88/88 [00:10<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0387, Val Loss: 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/2000: 100%|██████████| 88/88 [00:08<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0362, Val Loss: 0.1167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/2000: 100%|██████████| 88/88 [00:11<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0336, Val Loss: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/2000: 100%|██████████| 88/88 [00:10<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0321, Val Loss: 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/2000: 100%|██████████| 88/88 [00:08<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0314, Val Loss: 0.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/2000: 100%|██████████| 88/88 [00:09<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0308, Val Loss: 0.0790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/2000: 100%|██████████| 88/88 [00:10<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.0298, Val Loss: 0.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/2000: 100%|██████████| 88/88 [00:09<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.0300, Val Loss: 0.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/2000: 100%|██████████| 88/88 [00:09<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.0294, Val Loss: 0.0845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/2000: 100%|██████████| 88/88 [00:09<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.0289, Val Loss: 0.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/2000: 100%|██████████| 88/88 [00:08<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.0284, Val Loss: 0.1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/2000: 100%|██████████| 88/88 [00:10<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.0287, Val Loss: 0.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/2000: 100%|██████████| 88/88 [00:10<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.0285, Val Loss: 0.1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/2000: 100%|██████████| 88/88 [00:09<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.0277, Val Loss: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/2000: 100%|██████████| 88/88 [00:12<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.0276, Val Loss: 0.0998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/2000: 100%|██████████| 88/88 [00:20<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.0268, Val Loss: 0.0930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/2000: 100%|██████████| 88/88 [00:25<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.0267, Val Loss: 0.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/2000: 100%|██████████| 88/88 [00:11<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.0266, Val Loss: 0.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/2000: 100%|██████████| 88/88 [00:07<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.0268, Val Loss: 0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/2000: 100%|██████████| 88/88 [00:09<00:00,  8.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.0270, Val Loss: 0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/2000: 100%|██████████| 88/88 [00:11<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.0263, Val Loss: 0.1017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/2000: 100%|██████████| 88/88 [00:07<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss: 0.0260, Val Loss: 0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/2000: 100%|██████████| 88/88 [00:10<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss: 0.0257, Val Loss: 0.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/2000: 100%|██████████| 88/88 [00:09<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss: 0.0260, Val Loss: 0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/2000: 100%|██████████| 88/88 [00:22<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss: 0.0264, Val Loss: 0.0964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/2000: 100%|██████████| 88/88 [00:09<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss: 0.0258, Val Loss: 0.0801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/2000: 100%|██████████| 88/88 [00:07<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss: 0.0258, Val Loss: 0.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/2000: 100%|██████████| 88/88 [00:10<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss: 0.0259, Val Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/2000: 100%|██████████| 88/88 [00:10<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss: 0.0259, Val Loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/2000: 100%|██████████| 88/88 [00:09<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss: 0.0255, Val Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/2000:  28%|██▊       | 25/88 [00:03<00:07,  8.04it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load and preprocess data\n",
    "    df = load_data('Daily Data (1).csv')\n",
    "    df = add_features(df, window_size=7)\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y, feature_scaler, target_scaler = normalize_data(df)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_length=30)\n",
    "    \n",
    "    # Train-test split (time-series aware)\n",
    "    split_idx = int(0.8 * len(X_seq))\n",
    "    X_train, y_train = X_seq[:split_idx], y_seq[:split_idx]\n",
    "    X_test, y_test = X_seq[split_idx:], y_seq[split_idx:]\n",
    "    \n",
    "    # Further split train into train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = LSTDataset(X_train, y_train)\n",
    "    val_dataset = LSTDataset(X_val, y_val)\n",
    "    test_dataset = LSTDataset(X_test, y_test)\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X_train.shape[2]  # Number of features\n",
    "    model = LSTPredictionModel(input_size, hidden_size=128, num_layers=2, dropout=0.3)\n",
    "    \n",
    "    # Train model\n",
    "    # Now the model will run for exactly 200 epochs\n",
    "    trained_model, history = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        epochs=2000  # Will complete all 200 epochs\n",
    "        )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_true, y_pred = evaluate_model(trained_model, test_loader, target_scaler)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc073c-4758-4f70-8daa-bd7797aa6ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
